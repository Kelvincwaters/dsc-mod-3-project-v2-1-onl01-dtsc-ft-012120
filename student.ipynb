{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Kelvin Waters\n",
    "* Student pace: online-ds-ft-012120\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Abhineet Kulkarni\n",
    "* Blog post URL: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All imports will be moved here upon completion of the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required for the notebook\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import winsound\n",
    "\n",
    "from scipy import stats\n",
    "from importlib import reload\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, \n",
    "confusion_matrix, precision_score, f1_score, roc_curve, auc, plot_roc_curve, classification_report)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "from sklearn import svm, tree\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import pickle\n",
    "import sklearn.metrics as metrics\n",
    "import itertools\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all cell output \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "# first five rows of the dataset\n",
    "df.head() # normalized dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking initial shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good amount of data for ML\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all continuous except target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking note of min - max values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking outliers based on zscore < 3 value\n",
    "# not sure if taking zscore values on a normalized dataset\n",
    "z = np.abs(stats.zscore(df))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would remove all the 1 values from the target Class!\n",
    "# df = df[(z < 3).all(axis= 1)]\n",
    "# loss of 37,864 rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, dependant variable\n",
    "# data header 'cleaned' possibly for privacy concerns, would be great to know\n",
    "# what features we're dealing with here\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset is highly imbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "X = df.iloc[:, :-1] # all rows and minus the last column\n",
    "y = df.Class # only the Class column/Series as our target\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.20, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial shape pre SMOTING\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (on unbalanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "# default values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# generate predictions\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# how many times was the classifier correct on training?\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "\n",
    "# how many times correct on test set? \n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print('------------------------------------')\n",
    "print(pd.Series(residuals).value_counts(normalize= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression imbalanced\n",
    "confusion_matrix(y_test, y_hat_test)\n",
    "print('--------------------------------')\n",
    "print(classification_report(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression ROC Curve (imbalanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve imbalanced dataset\n",
    "\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(8, 6))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What options are available to address the imbalance? \n",
    "1. We can undersample '0' values, which would slash a HUGE amount of data points from the dataset. \n",
    "2. We can oversample '1' values which would double to the amount of data points on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE: synthetic minority oversampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a smote obj\n",
    "smote = SMOTE(random_state= 42) \n",
    "\n",
    "# apply smote to training data NOT the testing data\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# verifying the 'smoting' results!\n",
    "# more data points is preferable than fewer data points when it comes to machine learning\n",
    "print('Before SMOTE :', Counter(y_train))\n",
    "print('After SMOTE :', Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smote.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "tree = DecisionTreeClassifier(criterion= 'entropy', splitter= 'best')\n",
    "\n",
    "X_train_smote, X_test, y_train_smote, y_test = train_test_split(X, y, test_size= 0.20, random_state= 42)\n",
    "\n",
    "# fit model\n",
    "tree.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# predict\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "acc= accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "print('Accuracy is: {0}'.format(acc))\n",
    "\n",
    "# Check AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Confusion Matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print('----------------------------')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obscured feature v17 an imporatant variable would be good to know what this actually is!\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train_smote.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train_smote.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "plot_feature_importances(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Treee GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # default values are first in each dict list\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "# }\n",
    "# gs_tree = GridSearchCV(tree, param_grid, cv=5) # 5 K fold\n",
    "# gs_tree.fit(X_train_smote, y_train_smote)\n",
    "# gs_tree.best_params_\n",
    "\n",
    "# {'criterion': 'entropy', 'splitter': 'best'} results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deicsion Tree confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_test, y_hat_test)\n",
    "# print('Confusion Matrix:\\n', cnf_matrix)\n",
    "# print(classification_report(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "logreg_bal = LogisticRegression(penalty= 'l1', C= 1e12, solver= 'liblinear')\n",
    "\n",
    "# fit model\n",
    "logreg_bal.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# generate predictions\n",
    "y_hat_train = logreg_bal.predict(X_train_smote)\n",
    "y_hat_test = logreg_bal.predict(X_test)\n",
    "\n",
    "# how many times was the classifier correct on training?\n",
    "residuals = np.abs(y_train_smote - y_hat_train)\n",
    "\n",
    "# how many times correct on test set? \n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print('------------------------------------')\n",
    "print(pd.Series(residuals).value_counts(normalize= True))\n",
    "\n",
    "\n",
    "# default parameters\n",
    "# 0    55834\n",
    "# 1     1128\n",
    "# Name: Class, dtype: int64\n",
    "# ------------------------------------\n",
    "# 0    0.980197\n",
    "# 1    0.019803\n",
    "# Name: Class, dtype: float64\n",
    "# array([[55746,  1118],\n",
    "#        [   10,    88]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "confusion_matrix(y_test, y_hat_test)\n",
    "print('----------------------------')\n",
    "print(classification_report(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regresstion ROC curve (balance dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve imbalanced dataset\n",
    "\n",
    "y_score = logreg.fit(X_train_smote, y_train_smote).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(8, 6))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # default values are first in each dict list\n",
    "# param_grid = {\n",
    "#     'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "#     'C': [1.0, 1e12, 2e12],\n",
    "#     'solver': ['lbfgs', 'saga', 'liblinear'],\n",
    "# }\n",
    "# gs_logreg = GridSearchCV(logreg, param_grid, cv=5) # 5 K fold\n",
    "# gs_logreg.fit(X_train_smote, y_train_smote)\n",
    "# gs_logreg.best_params_\n",
    "\n",
    "# # {'C': 1000000000000.0, 'penalty': 'l1', 'solver': 'liblinear'} results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion= 'entropy', n_estimators= 100, n_jobs= None)\n",
    "\n",
    "# fit\n",
    "forest.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# predict\n",
    "forest_predict = forest.predict(X_test) # this can't be right, smote isn't applied to testing values\n",
    "\n",
    "# train accuracy score\n",
    "forest.score(X_train_smote, y_train_smote)\n",
    "\n",
    "# test accuracy score\n",
    "forest.score(X_test, y_test)      \n",
    "\n",
    "# default 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Classifier\n",
    "cnf_matrix = confusion_matrix(y_test, forest_predict) # y_test true, forest_predict\n",
    "print('Confusion Matrix:\\n', cnf_matrix)\n",
    "print('---------')\n",
    "print(classification_report(y_test, forest_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # default values are first in each dict list\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 100],\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'n_jobs': [None, -1]\n",
    "# }\n",
    "# gs_forest = GridSearchCV(forest, param_grid, cv=5) # 5 K fold\n",
    "# gs_forest.fit(X_train_smote, y_train_smote)\n",
    "# gs_forest.best_params_\n",
    "\n",
    "# {'criterion': 'entropy', 'n_estimators': 100, 'n_jobs': None} results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest ROC Curve\n",
    "# ax= plt.gca()\n",
    "\n",
    "# forest_disp = plot_roc_curve(forest, X_test, y_test, ax=ax, alpha=0.8)\n",
    "# svc_disp.plot(ax=ax, alpha=0.8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate XGBoost Classifier\n",
    "xgb = xgb.XGBClassifier(booster = 'gbtree')\n",
    "\n",
    "# fit\n",
    "xgb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# train\n",
    "training_pred = xgb.predict(X_train_smote)\n",
    "val_pred = xgb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "training_accuracy = accuracy_score(y_train_smote, training_pred)\n",
    "val_accuracy = accuracy_score(y_test, val_pred)\n",
    "\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, val_pred) # y_test being 'true' and val_pred being y_prediction\n",
    "print('Confusion Matrix:\\n', cnf_matrix)\n",
    "print('---------')\n",
    "print(classification_report(y_test, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell alarm when complete\n",
    "duration = 7000  # millisecond\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # default values are first in each dict list\n",
    "# param_grid = {\n",
    "#     'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "# }\n",
    "# gs_xgb = GridSearchCV(xgb, param_grid, cv=5) # 5 K fold\n",
    "# gs_xgb.fit(X_train_smote, y_train_smote)\n",
    "# gs_xgb.best_params_\n",
    "\n",
    "# {'booster': 'gbtree'} # results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abandoned! SMOTE via pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This would never complete when referencing the created balanced dataset, google search revealed that there \n",
    "maybe issues with smote running on a single core, could not find any workarounds. I'm not even using column_transformer \n",
    "for dummies or any type of scaler. There's an apparent bottle neck somewhere since it runs without issue on the imbalanced \n",
    "data! I suspect an issue with SMOTE\"\"\"\n",
    "\n",
    "# # logistic Regression\n",
    "# pipe_lr = Pipeline([('pca', PCA(n_components= 2)),\n",
    "#             ('clf', LogisticRegression(random_state= 42))])\n",
    "# # SVM\n",
    "# pipe_svm = Pipeline([('pca', PCA(n_components= 2)),\n",
    "#             ('clf', svm.SVC(random_state= 42))])\n",
    "# # Decision Tree            \n",
    "# pipe_dt = Pipeline([('pca', PCA(n_components= 2)),\n",
    "#             ('clf', tree.DecisionTreeClassifier(random_state= 42))])\n",
    "\n",
    "\n",
    "# # List of pipelines for ease of iteration\n",
    "# pipelines = [pipe_lr, pipe_svm, pipe_dt]\n",
    "\n",
    "# # Dictionary of pipelines and classifier types for ease of reference\n",
    "# pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Machine', 2: 'Decision Tree'}\n",
    "\n",
    "# # Fit the pipelines\n",
    "# for pipe in pipelines:\n",
    "#     pipe.fit(X_train_smote, y_train_smote) # smote taking extremely long\n",
    "\n",
    "# # Compare accuracies\n",
    "# for idx, val in enumerate(pipelines):\n",
    "#     print('%s pipeline test accuracy: %.3f' % (pipe_dict[idx], val.score(X_test, y_test)))\n",
    "\n",
    "# # Identify the most accurate model on test data\n",
    "# best_acc = 0.0\n",
    "# best_clf = 0\n",
    "# best_pipe = ''\n",
    "# for idx, val in enumerate(pipelines):\n",
    "#     if val.score(X_test, y_test) > best_acc:\n",
    "#         best_acc = val.score(X_test, y_test)\n",
    "#         best_pipe = val\n",
    "#         best_clf = idx\n",
    "# print('Classifier with best accuracy: %s' % pipe_dict[best_clf])\n",
    "\n",
    "# # Save pipeline to file\n",
    "# joblib.dump(best_pipe, 'best_pipeline.pkl', compress=1)\n",
    "# print('Saved %s pipeline to file' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pickle\n",
    "# with open('best_pipeline.pkl', 'rb') as p_f:\n",
    "#     data = pickle.load(p_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
